{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eef_sLpwEV10"
   },
   "source": [
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "So_EkN01UIlq"
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26421,
     "status": "ok",
     "timestamp": 1616570513050,
     "user": {
      "displayName": "Alex Wey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0JtGJkQBcl8BqawYKxa9JOOKHAjJzj0pcPtA42g=s64",
      "userId": "16845934267555865349"
     },
     "user_tz": 240
    },
    "id": "hLoPyCpKJ8fj",
    "outputId": "9ef06cfb-ba64-4b41-d5bb-2fb63a8488ee"
   },
   "outputs": [],
   "source": [
    "path = '/Users/alexwey/Desktop/ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31274,
     "status": "ok",
     "timestamp": 1616570517910,
     "user": {
      "displayName": "Alex Wey",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh0JtGJkQBcl8BqawYKxa9JOOKHAjJzj0pcPtA42g=s64",
      "userId": "16845934267555865349"
     },
     "user_tz": 240
    },
    "id": "XLAm2umcVTUa",
    "outputId": "86ca0579-cd0f-4892-ff7b-09a8b2b1d6c4"
   },
   "outputs": [],
   "source": [
    "# Import all json files\n",
    "data = defaultdict()\n",
    "\n",
    "d = json.JSONDecoder()\n",
    "with open(path + \"/GPT3_Gen.jsonl\", 'r') as file:\n",
    "    for i, record in enumerate(file):\n",
    "        record_data = d.decode(record)\n",
    "        url = record_data['url']\n",
    "        domain = record_data['domain']\n",
    "        publish_date = record_data['publish_date']\n",
    "        text = record_data['text']\n",
    "        generation = record_data['generation'].strip()\n",
    "        data[i] = ('g', url, domain, publish_date, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_txt(path, id, doc):\n",
    "    text_file = open(f\"{path}/04_27_21/{id}.txt\", \"w\")\n",
    "    text_file.write(doc)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(tup):\n",
    "    type_doc = tup[0]\n",
    "    domain = tup[2]\n",
    "    publish_date = tup[3]\n",
    "    doc = tup[4]\n",
    "    return type_doc, domain, publish_date, doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Share articles\n",
    "all_keys = []\n",
    "share = random.sample(list(data), 3)\n",
    "a_path = f'/Users/alexwey/Desktop/CS/GPT-2/Grover/gpt-2/annotations/'\n",
    "df = [[\"COLLECTED\", \"ON\", datetime.date(datetime.now()), None, None]]\n",
    "index = 0\n",
    "\n",
    "# First shared docs\n",
    "first = data[share[0]]\n",
    "type_doc, domain, publish_date, doc = collect_data(first)\n",
    "\n",
    "if type_doc == 'o':\n",
    "    id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "    df.append([id, publish_date, index, \"original\", domain])\n",
    "    write_txt(a_path + 'a0', id, doc)\n",
    "    write_txt(a_path + 'a1', id, doc)\n",
    "elif type_doc == 'g':\n",
    "    id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "    df.append([id, publish_date, index, \"grover\", domain])\n",
    "    write_txt(a_path + 'a0', id, doc)\n",
    "    write_txt(a_path + 'a1', id, doc)\n",
    "\n",
    "index += 1\n",
    "time.sleep(1)\n",
    "\n",
    "# Second shared docs\n",
    "second = data[share[1]]\n",
    "type_doc, domain, publish_date, doc = collect_data(second)\n",
    "\n",
    "if type_doc == 'o':\n",
    "    id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "    df.append([id, publish_date, index, \"original\", domain])\n",
    "    write_txt(a_path + 'a1', id, doc)\n",
    "    write_txt(a_path + 'a2', id, doc)\n",
    "elif type_doc == 'g':\n",
    "    id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "    df.append([id, publish_date, index, \"grover\", domain])\n",
    "    write_txt(a_path + 'a1', id, doc)\n",
    "    write_txt(a_path + 'a2', id, doc)\n",
    "\n",
    "index += 1\n",
    "time.sleep(1)\n",
    "\n",
    "# Third shared docs\n",
    "third = data[share[2]]\n",
    "type_doc, domain, publish_date, doc = collect_data(third)\n",
    "\n",
    "if type_doc == 'o':\n",
    "    id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "    df.append([id, publish_date, index, \"original\", domain])\n",
    "    write_txt(a_path + 'a2', id, doc)\n",
    "    write_txt(a_path + 'a0', id, doc)\n",
    "elif type_doc == 'g':\n",
    "    id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "    df.append([id, publish_date, index, \"grover\", domain])\n",
    "    write_txt(a_path + 'a2', id, doc)\n",
    "    write_txt(a_path + 'a0', id, doc)\n",
    "\n",
    "index += 1\n",
    "\n",
    "# Remove keys in share\n",
    "for key in share:\n",
    "    data.pop(key, None)\n",
    "    all_keys.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribute articles\n",
    "a0_keys = random.sample(list(data), 15)\n",
    "for key in a0_keys:\n",
    "    type_doc, domain, publish_date, doc = collect_data(data[key])\n",
    "    if type_doc == 'o':\n",
    "        id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        df.append([id, publish_date, index, \"original\", domain])\n",
    "        write_txt(a_path + 'a0', id, doc)\n",
    "    elif type_doc == 'g':\n",
    "        id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        df.append([id, publish_date, index, \"grover\", domain])\n",
    "        write_txt(a_path + 'a0', id, doc)\n",
    "\n",
    "    data.pop(key, None)\n",
    "    all_keys.append(key)\n",
    "    index += 1\n",
    "    time.sleep(1)\n",
    "\n",
    "a1_keys = random.sample(list(data), 15)\n",
    "for key in a1_keys:\n",
    "    type_doc, domain, publish_date, doc = collect_data(data[key])\n",
    "    if type_doc == 'o':\n",
    "        id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        df.append([id, publish_date, index, \"original\", domain])\n",
    "        write_txt(a_path + 'a1', id, doc)\n",
    "    elif type_doc == 'g':\n",
    "        id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        df.append([id, publish_date, index, \"grover\", domain])\n",
    "        write_txt(a_path + 'a1', id, doc)\n",
    "    \n",
    "    data.pop(key, None)\n",
    "    all_keys.append(key)\n",
    "    index += 1\n",
    "    time.sleep(1)\n",
    "    \n",
    "a2_keys = random.sample(list(data), 15)\n",
    "for key in a2_keys:\n",
    "    type_doc, domain, publish_date, doc = collect_data(data[key])\n",
    "    if type_doc == 'o':\n",
    "        id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        df.append([id, publish_date, index, \"original\", domain])\n",
    "        write_txt(a_path + 'a2', id, doc)\n",
    "    elif type_doc == 'g':\n",
    "        id = datetime.now().strftime(\"%d%m%y%H%M%S\")\n",
    "        df.append([id, publish_date, index, \"grover\", domain])\n",
    "        write_txt(a_path + 'a2', id, doc)\n",
    "    \n",
    "    data.pop(key, None)\n",
    "    all_keys.append(key)\n",
    "    index += 1\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(df, columns=[\"id\", \"publish_date\", \"index\", \"type\", \"domain\"])\n",
    "df2 = pd.read_csv(\"/Users/alexwey/Desktop/CS/GPT-2/Grover/gpt-2/info/info.csv\")\n",
    "df = df1.append(df2)\n",
    "df.to_csv(f\"/Users/alexwey/Desktop/CS/GPT-2/Grover/gpt-2/info/info.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Eef_sLpwEV10",
    "UoV8btM4Ec3G",
    "Mnr6AOvNJCuL",
    "Df3Y5_LKZYQ6",
    "BH6qEssJXZlR",
    "GdOzIm_prDJA",
    "S4Gb5uRZrITe",
    "SqQeFccorPw8",
    "aJcIn9zsrLrg",
    "Wx_d8x9KrUBM",
    "BgHJlgJ2_c2o",
    "aI1rM9lc81p0"
   ],
   "name": "Human_Grover_News.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
